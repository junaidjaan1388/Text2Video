name: Text-to-Video Generator

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt for video generation'
        required: true
        default: 'A beautiful sunset over mountains, cinematic, 4k'
      model:
        description: 'Video generation model to use'
        required: true
        default: 'damo-vilab/text-to-video-ms-1.7b'
        type: choice
        options:
          - 'damo-vilab/text-to-video-ms-1.7b'
          - 'cerspense/zeroscope_v2_576w'
          - 'anotherjesse/zeroscope-v2-xl'
          - 'camenduru/text-to-video'
      num_frames:
        description: 'Number of frames to generate'
        required: true
        default: 24
        type: number
      num_inference_steps:
        description: 'Number of inference steps'
        required: true
        default: 50
        type: number
      height:
        description: 'Video height'
        required: true
        default: 256
        type: number
      width:
        description: 'Video width'
        required: true
        default: 256
        type: number

jobs:
  generate-video:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libgl1 \
          libglib2.0-0 \
          ffmpeg \
          git-lfs \
          wget

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        pip install diffusers transformers accelerate
        pip install opencv-python pillow imageio-ffmpeg
        pip install numpy decord av
        pip install huggingface_hub

    - name: Create outputs directory
      run: |
        mkdir -p outputs/videos
        mkdir -p outputs/frames
        echo "Directories created"

    - name: Debug information
      run: |
        echo "=== System Information ==="
        nproc
        free -h
        df -h
        
        echo "=== Python Information ==="
        python --version
        pip list | grep -E "(torch|diffusers|transformers)"

    - name: Run text-to-video generation
      id: generate_video
      env:
        HUGGINGFACE_HUB_CACHE: /tmp/huggingface_cache
        PROMPT: ${{ github.event.inputs.prompt }}
        MODEL: ${{ github.event.inputs.model }}
        NUM_FRAMES: ${{ github.event.inputs.num_frames }}
        NUM_INFERENCE_STEPS: ${{ github.event.inputs.num_inference_steps }}
        HEIGHT: ${{ github.event.inputs.height }}
        WIDTH: ${{ github.event.inputs.width }}
      run: |
        echo "Starting text-to-video generation..."
        echo "Prompt: $PROMPT"
        echo "Model: $MODEL"
        echo "Frames: $NUM_FRAMES"
        echo "Steps: $NUM_INFERENCE_STEPS"
        echo "Resolution: ${WIDTH}x${HEIGHT}"
        
        # Create the video generation script
        cat > text_to_video_generator.py << 'EOF'
import os
import torch
import numpy as np
from diffusers import DiffusionPipeline
from PIL import Image
import imageio
import datetime
import traceback

def create_test_video(prompt, width, height, num_frames):
    """Create a test video with moving elements"""
    print("Creating test video...")
    
    frames = []
    for i in range(num_frames):
        # Create a frame with moving elements
        img = Image.new('RGB', (width, height), color='lightblue')
        
        # Draw moving elements
        from PIL import ImageDraw
        draw = ImageDraw.Draw(img)
        
        # Animated sun
        sun_x = width // 2
        sun_y = height // 4 + (i * 2) % (height // 2)
        draw.ellipse([sun_x-20, sun_y-20, sun_x+20, sun_y+20], fill='yellow')
        
        # Moving text
        text_x = (i * 5) % width
        draw.text((text_x, height-50), f"Prompt: {prompt}", fill='black')
        draw.text((50, 30), f"Frame: {i+1}/{num_frames}", fill='black')
        draw.text((50, 60), "Test Video - AI Model", fill='black')
        draw.text((50, 90), f"Time: {datetime.datetime.now()}", fill='black')
        
        # Convert to numpy array for video
        frames.append(np.array(img))
    
    return frames

def main():
    try:
        # Get environment variables
        prompt = os.getenv('PROMPT', 'A beautiful sunset')
        model_id = os.getenv('MODEL', 'damo-vilab/text-to-video-ms-1.7b')
        num_frames = int(os.getenv('NUM_FRAMES', 24))
        num_inference_steps = int(os.getenv('NUM_INFERENCE_STEPS', 50))
        height = int(os.getenv('HEIGHT', 256))
        width = int(os.getenv('WIDTH', 256))
        
        print(f"Video Generation Parameters:")
        print(f"  Prompt: {prompt}")
        print(f"  Model: {model_id}")
        print(f"  Frames: {num_frames}")
        print(f"  Steps: {num_inference_steps}")
        print(f"  Resolution: {width}x{height}")
        
        # Create output directories
        os.makedirs('outputs/videos', exist_ok=True)
        os.makedirs('outputs/frames', exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Try to use actual diffusion model, fallback to test video
        try:
            print("Loading diffusion model...")
            
            # Use CPU for compatibility
            device = "cpu"
            torch_dtype = torch.float32
            
            # Load the appropriate pipeline
            if "zeroscope" in model_id.lower():
                from diffusers import DiffusionPipeline
                pipe = DiffusionPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch_dtype
                )
            else:
                from diffusers import DiffusionPipeline
                pipe = DiffusionPipeline.from_pretrained(
                    model_id,
                    torch_dtype=torch_dtype,
                    trust_remote_code=True
                )
            
            pipe = pipe.to(device)
            
            print("Generating video with diffusion model...")
            # Generate video frames
            video_frames = pipe(
                prompt,
                num_frames=num_frames,
                num_inference_steps=num_inference_steps,
                height=height,
                width=width
            ).frames
            
        except Exception as e:
            print(f"Diffusion model failed: {e}")
            print("Falling back to test video generation...")
            video_frames = create_test_video(prompt, width, height, num_frames)
        
        # Save as GIF
        gif_path = f"outputs/videos/generated_video_{timestamp}.gif"
        print(f"Saving GIF: {gif_path}")
        
        # Convert frames to PIL Images if they aren't already
        pil_frames = []
        for frame in video_frames:
            if isinstance(frame, np.ndarray):
                pil_frames.append(Image.fromarray(frame))
            else:
                pil_frames.append(frame)
        
        # Save as GIF
        pil_frames[0].save(
            gif_path,
            save_all=True,
            append_images=pil_frames[1:],
            duration=100,  # ms per frame
            loop=0
        )
        
        # Save individual frames
        for i, frame in enumerate(pil_frames):
            frame_path = f"outputs/frames/frame_{i:04d}.png"
            frame.save(frame_path)
        
        # Create info file
        info_content = f"""Video Generation Results
Generated: {datetime.datetime.now()}
Prompt: {prompt}
Model: {model_id}
Frames: {num_frames}
Steps: {num_inference_steps}
Resolution: {width}x{height}
Output Files:
  - {gif_path}
  - outputs/frames/ (individual frames)
"""
        with open("outputs/generation_info.txt", "w") as f:
            f.write(info_content)
        
        print("Video generation completed successfully!")
        print(f"Output: {gif_path}")
        
    except Exception as e:
        print(f"Error in video generation: {e}")
        print(traceback.format_exc())
        
        # Create error log
        os.makedirs('outputs', exist_ok=True)
        with open('outputs/error_log.txt', 'w') as f:
            f.write(f"Error: {e}\n")
            f.write(traceback.format_exc())
        
        raise

if __name__ == "__main__":
    main()
EOF

        # Run the video generation script
        python text_to_video_generator.py

    - name: Verify generated files
      run: |
        echo "=== Checking outputs ==="
        find outputs/ -type f | while read file; do
          echo "File: $file ($(wc -c < "$file") bytes)"
        done
        
        echo "=== Video files ==="
        find outputs/videos/ -type f 2>/dev/null || echo "No video files found"
        
        echo "=== Frame files ==="
        find outputs/frames/ -type f 2>/dev/null | head -10
        echo "... (showing first 10 frames)"

    - name: Create preview thumbnail
      run: |
        # Create a preview image from the first frame
        if [ -f outputs/frames/frame_0000.png ]; then
          cp outputs/frames/frame_0000.png outputs/video_preview.png
          echo "Preview thumbnail created"
        else
          # Create a simple preview
          python -c "
          from PIL import Image, ImageDraw
          img = Image.new('RGB', (300, 200), color='lightblue')
          d = ImageDraw.Draw(img)
          d.text((50, 80), 'Video Preview', fill='black')
          d.text((50, 110), 'Frame extraction failed', fill='red')
          img.save('outputs/video_preview.png')
          "
          echo "Fallback preview created"
        fi

    - name: Upload generated videos and frames
      uses: actions/upload-artifact@v4
      with:
        name: generated-videos
        path: |
          outputs/videos/
          outputs/frames/
          outputs/video_preview.png
          outputs/generation_info.txt
        retention-days: 30
        if-no-files-found: error

    - name: Upload error logs if generation failed
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: generation-error-logs
        path: outputs/
        retention-days: 7

    - name: Success notification
      if: success()
      run: |
        echo "üéâ Video generation completed successfully!"
        echo "üìπ Generated videos and frames are available as artifacts"
        echo "‚è∞ Total frames: ${{ github.event.inputs.num_frames }}"
        echo "üñºÔ∏è Resolution: ${{ github.event.inputs.width }}x${{ github.event.inputs.height }}"
