name: Text-to-Video Generator

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt for video generation'
        required: true
        default: 'A beautiful sunset over mountains, cinematic, 4k'
      model:
        description: 'Video generation model to use'
        required: true
        default: 'damo-vilab/text-to-video-ms-1.7b'
        type: choice
        options:
          - 'damo-vilab/text-to-video-ms-1.7b'
          - 'cerspense/zeroscope_v2_576w'
          - 'anotherjesse/zeroscope-v2-xl'
      num_frames:
        description: 'Number of frames to generate'
        required: true
        default: 16
        type: number
      num_inference_steps:
        description: 'Number of inference steps'
        required: true
        default: 25
        type: number
      height:
        description: 'Video height'
        required: true
        default: 256
        type: number
      width:
        description: 'Video width'
        required: true
        default: 256
        type: number

jobs:
  generate-video:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libgl1 \
          libglib2.0-0 \
          ffmpeg \
          git-lfs \
          wget \
          pkg-config

    - name: Configure Git LFS
      run: |
        git lfs install

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        pip install diffusers[torch]==0.21.4 transformers accelerate
        pip install opencv-python pillow imageio imageio-ffmpeg
        pip install numpy decord av
        pip install huggingface_hub xformers

    - name: Create outputs directory
      run: |
        mkdir -p outputs/videos
        mkdir -p outputs/frames
        echo "Directories created"

    - name: Debug information
      run: |
        echo "=== System Information ==="
        nproc
        free -h
        df -h
        echo "=== Python Packages ==="
        pip list | grep -E "(torch|diffusers|transformers|accelerate)"

    - name: Run text-to-video generation
      id: generate_video
      env:
        HF_HUB_ENABLE_HF_TRANSFER: 1
        PROMPT: ${{ github.event.inputs.prompt }}
        MODEL: ${{ github.event.inputs.model }}
        NUM_FRAMES: ${{ github.event.inputs.num_frames }}
        NUM_INFERENCE_STEPS: ${{ github.event.inputs.num_inference_steps }}
        HEIGHT: ${{ github.event.inputs.height }}
        WIDTH: ${{ github.event.inputs.width }}
      run: |
        echo "Starting text-to-video generation..."
        echo "Prompt: $PROMPT"
        echo "Model: $MODEL"
        echo "Frames: $NUM_FRAMES"
        echo "Steps: $NUM_INFERENCE_STEPS"
        echo "Resolution: ${WIDTH}x${HEIGHT}"
        
        # Create the improved video generation script
        cat > text_to_video_generator.py << 'EOF'
import os
import torch
import numpy as np
from PIL import Image, ImageDraw
import imageio
import datetime
import traceback
import sys

def create_advanced_test_video(prompt, width, height, num_frames):
    """Create an advanced test video with better animations"""
    print("Creating advanced test video...")
    
    frames = []
    for i in range(num_frames):
        # Create gradient background
        img = Image.new('RGB', (width, height), color='black')
        draw = ImageDraw.Draw(img)
        
        # Animated gradient
        for y in range(height):
            r = int(100 + 100 * np.sin(2 * np.pi * (i/num_frames + y/height)))
            g = int(100 + 100 * np.sin(2 * np.pi * (i/num_frames + y/height + 1/3)))
            b = int(100 + 100 * np.sin(2 * np.pi * (i/num_frames + y/height + 2/3)))
            draw.line([(0, y), (width, y)], fill=(r, g, b))
        
        # Moving shapes
        circle_x = width // 4 + int(width // 2 * np.sin(2 * np.pi * i / num_frames))
        circle_y = height // 3
        draw.ellipse([circle_x-30, circle_y-30, circle_x+30, circle_y+30], fill='red', outline='white')
        
        # Rotating square
        square_size = 40
        square_x = width * 3 // 4 + int(20 * np.cos(2 * np.pi * i / num_frames))
        square_y = height // 3 + int(20 * np.sin(2 * np.pi * i / num_frames))
        draw.rectangle([square_x-square_size//2, square_y-square_size//2, 
                       square_x+square_size//2, square_y+square_size//2], 
                      fill='blue', outline='white')
        
        # Text with shadow
        text_lines = [
            f"Prompt: {prompt[:50]}{'...' if len(prompt) > 50 else ''}",
            f"Model: Text-to-Video AI",
            f"Frame: {i+1}/{num_frames}",
            f"Time: {datetime.datetime.now().strftime('%H:%M:%S')}"
        ]
        
        for j, line in enumerate(text_lines):
            # Shadow
            draw.text((52, height-120 + j*25), line, fill='black')
            # Main text
            draw.text((50, height-122 + j*25), line, fill='white')
        
        frames.append(np.array(img))
    
    return frames

def load_diffusion_model(model_id, device):
    """Load the appropriate diffusion model"""
    try:
        from diffusers import DiffusionPipeline
        
        print(f"Loading model: {model_id}")
        
        # Use smaller models for CPU compatibility
        if "zeroscope" in model_id.lower():
            pipe = DiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float32,
                use_safetensors=True
            )
        else:
            pipe = DiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float32,
                use_safetensors=True
            )
        
        pipe = pipe.to(device)
        pipe.enable_attention_slicing()  # Reduce memory usage
        
        return pipe
        
    except Exception as e:
        print(f"Model loading failed: {e}")
        return None

def main():
    try:
        # Get environment variables
        prompt = os.getenv('PROMPT', 'A beautiful sunset')
        model_id = os.getenv('MODEL', 'damo-vilab/text-to-video-ms-1.7b')
        num_frames = int(os.getenv('NUM_FRAMES', 16))
        num_inference_steps = int(os.getenv('NUM_INFERENCE_STEPS', 25))
        height = int(os.getenv('HEIGHT', 256))
        width = int(os.getenv('WIDTH', 256))
        
        print(f"üé¨ Video Generation Parameters:")
        print(f"  Prompt: {prompt}")
        print(f"  Model: {model_id}")
        print(f"  Frames: {num_frames}")
        print(f"  Steps: {num_inference_steps}")
        print(f"  Resolution: {width}x{height}")
        
        # Create output directories
        os.makedirs('outputs/videos', exist_ok=True)
        os.makedirs('outputs/frames', exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Use CPU for GitHub Actions compatibility
        device = "cpu"
        video_frames = None
        
        # Try to use actual diffusion model
        if num_frames <= 24:  # Limit frames for CPU
            pipe = load_diffusion_model(model_id, device)
            if pipe is not None:
                try:
                    print("üöÄ Generating video with diffusion model...")
                    # Generate video with reduced parameters for CPU
                    generator = torch.Generator(device=device).manual_seed(42)
                    
                    video_result = pipe(
                        prompt,
                        num_frames=min(num_frames, 16),  # Further limit for stability
                        num_inference_steps=min(num_inference_steps, 30),
                        height=height,
                        width=width,
                        generator=generator
                    )
                    
                    if hasattr(video_result, 'frames'):
                        video_frames = video_result.frames
                        print("‚úÖ Diffusion model generation successful!")
                    else:
                        print("‚ùå No frames in model output")
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Diffusion generation failed: {e}")
        
        # Fallback to test video if model failed
        if video_frames is None:
            print("üîÑ Falling back to test video generation...")
            video_frames = create_advanced_test_video(prompt, width, height, num_frames)
        
        # Save as GIF
        gif_path = f"outputs/videos/generated_video_{timestamp}.gif"
        mp4_path = f"outputs/videos/generated_video_{timestamp}.mp4"
        
        print(f"üíæ Saving outputs...")
        
        # Convert frames to PIL Images if needed
        pil_frames = []
        for i, frame in enumerate(video_frames):
            if isinstance(frame, np.ndarray):
                pil_frame = Image.fromarray(frame)
            else:
                pil_frame = frame
            
            pil_frames.append(pil_frame)
            
            # Save individual frames
            frame_path = f"outputs/frames/frame_{i:04d}.png"
            pil_frame.save(frame_path)
        
        # Save as GIF
        pil_frames[0].save(
            gif_path,
            save_all=True,
            append_images=pil_frames[1:],
            duration=150,  # ms per frame
            loop=0,
            optimize=True
        )
        
        # Try to save as MP4 using imageio
        try:
            with imageio.get_writer(mp4_path, fps=8) as writer:
                for frame in pil_frames:
                    writer.append_data(np.array(frame))
            print(f"üìπ MP4 saved: {mp4_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è MP4 saving failed: {e}")
        
        # Create detailed info file
        info_content = f"""üé¨ VIDEO GENERATION RESULTS

üìÖ Generated: {datetime.datetime.now()}
üéØ Prompt: {prompt}
ü§ñ Model: {model_id}
üìä Frames: {num_frames}
‚öôÔ∏è Steps: {num_inference_steps}
üñºÔ∏è Resolution: {width}x{height}

üìÅ OUTPUT FILES:
  - GIF: {gif_path}
  - MP4: {mp4_path}
  - Frames: outputs/frames/ ({len(pil_frames)} frames)

üíæ File Sizes:
  - GIF: {os.path.getsize(gif_path) if os.path.exists(gif_path) else 0} bytes
  - MP4: {os.path.getsize(mp4_path) if os.path.exists(mp4_path) else 0} bytes
  - Total frames: {len(os.listdir('outputs/frames')) if os.path.exists('outputs/frames') else 0}

‚úÖ Generation: SUCCESS
"""
        with open("outputs/generation_info.txt", "w") as f:
            f.write(info_content)
        
        print("üéâ Video generation completed successfully!")
        print(f"üìÅ Output: {gif_path}")
        if os.path.exists(mp4_path):
            print(f"üìÅ Output: {mp4_path}")
        
    except Exception as e:
        print(f"‚ùå Error in video generation: {e}")
        print(traceback.format_exc())
        
        # Create error log
        os.makedirs('outputs', exist_ok=True)
        with open('outputs/error_log.txt', 'w') as f:
            f.write(f"Error: {str(e)}\n")
            f.write(traceback.format_exc())
        
        # Create at least a basic output
        try:
            basic_img = Image.new('RGB', (100, 100), color='red')
            basic_img.save('outputs/error_preview.png')
        except:
            pass
            
        sys.exit(1)

if __name__ == "__main__":
    main()
EOF

        echo "Running video generation script..."
        python text_to_video_generator.py

    - name: Verify and display generated files
      run: |
        echo "=== üóÇÔ∏è Generated Files ==="
        find outputs/ -type f -exec ls -lh {} \; 2>/dev/null || echo "No files found"
        
        echo "=== üìä File Sizes ==="
        du -sh outputs/ || echo "Cannot check directory size"
        
        echo "=== üé¨ Video Files ==="
        find outputs/videos/ -name "*.gif" -o -name "*.mp4" 2>/dev/null | while read file; do
          echo "Video: $file ($(stat -c%s "$file" 2>/dev/null || echo "unknown") bytes)"
        done

    - name: Create preview thumbnail
      run: |
        # Try to create preview from first frame
        if ls outputs/frames/frame_0000.png 1> /dev/null 2>&1; then
          cp outputs/frames/frame_0000.png outputs/video_preview.png
          echo "‚úÖ Preview thumbnail created from first frame"
        else
          # Create fallback preview
          python3 -c "
from PIL import Image, ImageDraw, ImageFont
img = Image.new('RGB', (400, 300), color='#1a1a2e')
d = ImageDraw.Draw(img)
d.rectangle([50, 50, 350, 250], outline='#4cc9f0', width=3)
d.text((100, 130), 'üé¨ Video Generated', fill='#4cc9f0')
d.text((80, 160), 'Download artifacts below', fill='#f72585')
img.save('outputs/video_preview.png')
print('‚úÖ Fallback preview created')
"
        fi

    - name: Upload generated videos
      uses: actions/upload-artifact@v4
      with:
        name: generated-videos
        path: |
          outputs/videos/
          outputs/frames/
          outputs/video_preview.png
          outputs/generation_info.txt
        retention-days: 30
        if-no-files-found: error
        compression-level: 9

    - name: Upload debug info on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: debug-logs
        path: outputs/
        retention-days: 7

    - name: Success notification
      if: success()
      run: |
        echo "üéâ Video generation completed successfully!"
        echo "üìπ Check the 'Artifacts' section to download your generated videos"
        echo "üñºÔ∏è Resolution: ${{ github.event.inputs.width }}x${{ github.event.inputs.height }}"
        echo "üìä Frames: ${{ github.event.inputs.num_frames }}"
        echo "‚öôÔ∏è Model: ${{ github.event.inputs.model }}"
