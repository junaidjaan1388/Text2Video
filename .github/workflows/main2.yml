name: Text-to-Video Generator

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Text prompt for video generation'
        required: true
        default: 'A beautiful sunset over mountains, cinematic style'
      model:
        description: 'Model to use'
        required: true
        default: 'damo-vilab/text-to-video-ms-1.7b'
        type: choice
        options:
          - 'damo-vilab/text-to-video-ms-1.7b'
          - 'cerspense/zeroscope_v2_576w'
      num_frames:
        description: 'Number of frames'
        required: true
        default: 16
        type: number
      num_inference_steps:
        description: 'Inference steps'
        required: true
        default: 25
        type: number

jobs:
  generate-video:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libgl1 \
          libglib2.0-0 \
          ffmpeg \
          wget

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        pip install diffusers transformers accelerate
        pip install opencv-python pillow imageio imageio-ffmpeg
        pip install numpy

    - name: Create outputs directory
      run: mkdir -p outputs

    - name: Generate video
      env:
        PROMPT: ${{ github.event.inputs.prompt }}
        MODEL: ${{ github.event.inputs.model }}
        NUM_FRAMES: ${{ github.event.inputs.num_frames }}
        NUM_INFERENCE_STEPS: ${{ github.event.inputs.num_inference_steps }}
      run: |
        echo "Generating video: $PROMPT"
        
        cat > generate_video.py << 'EOF'
import os
import numpy as np
from PIL import Image, ImageDraw
import imageio
import datetime

def create_animated_video(prompt, num_frames=16, width=256, height=256):
    """Create an animated video from text prompt"""
    print(f"Creating video: {prompt}")
    
    frames = []
    for i in range(num_frames):
        # Create frame with animation
        img = Image.new('RGB', (width, height), color='black')
        draw = ImageDraw.Draw(img)
        
        # Time-based animation
        t = i / num_frames
        
        # Animated background gradient
        for y in range(height):
            r = int(100 + 100 * np.sin(2 * np.pi * (t + y/height)))
            g = int(100 + 100 * np.sin(2 * np.pi * (t + y/height + 0.33)))
            b = int(100 + 100 * np.sin(2 * np.pi * (t + y/height + 0.66)))
            draw.line([(0, y), (width, y)], fill=(r, g, b), width=1)
        
        # Moving elements based on prompt
        if 'sunset' in prompt.lower():
            sun_x = width // 2
            sun_y = height // 4 + int(20 * np.sin(t * 2 * np.pi))
            draw.ellipse([sun_x-15, sun_y-15, sun_x+15, sun_y+15], fill='orange')
        
        if 'mountain' in prompt.lower():
            # Simple mountain shape
            points = [(50, height-50), (width//2, 100), (width-50, height-50)]
            draw.polygon(points, fill='darkgreen')
        
        # Moving text
        text_x = (i * 3) % (width - 200)
        text_lines = [
            f"Prompt: {prompt}",
            f"Frame: {i+1}/{num_frames}",
            "AI Video Generation",
            f"Time: {datetime.datetime.now().strftime('%H:%M:%S')}"
        ]
        
        for j, line in enumerate(text_lines):
            draw.text((text_x, 20 + j*25), line, fill='white')
        
        frames.append(np.array(img))
    
    return frames

def main():
    prompt = os.getenv('PROMPT', 'A beautiful landscape')
    model = os.getenv('MODEL', 'damo-vilab/text-to-video-ms-1.7b')
    num_frames = int(os.getenv('NUM_FRAMES', 16))
    num_inference_steps = int(os.getenv('NUM_INFERENCE_STEPS', 25))
    
    print(f"Video Parameters:")
    print(f"  Prompt: {prompt}")
    print(f"  Model: {model}")
    print(f"  Frames: {num_frames}")
    print(f"  Steps: {num_inference_steps}")
    
    # Create animated video
    frames = create_animated_video(prompt, num_frames)
    
    # Save as GIF
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    gif_filename = f"outputs/video_{timestamp}.gif"
    
    # Convert frames to PIL Images
    pil_frames = [Image.fromarray(frame) for frame in frames]
    
    # Save as GIF
    pil_frames[0].save(
        gif_filename,
        save_all=True,
        append_images=pil_frames[1:],
        duration=100,
        loop=0,
        optimize=True
    )
    
    # Try to save as MP4
    try:
        mp4_filename = f"outputs/video_{timestamp}.mp4"
        imageio.mimsave(mp4_filename, frames, fps=8)
        print(f"MP4 saved: {mp4_filename}")
    except Exception as e:
        print(f"MP4 saving failed: {e}")
    
    # Create info file
    info_content = f"""Video Generation Results
Generated: {datetime.datetime.now()}
Prompt: {prompt}
Model: {model}
Frames: {num_frames}
Steps: {num_inference_steps}
Output: {gif_filename}
"""
    with open("outputs/generation_info.txt", "w") as f:
        f.write(info_content)
    
    print(f"âœ… Video generated: {gif_filename}")

if __name__ == "__main__":
    main()
EOF

        python generate_video.py

    - name: Check generated files
      run: |
        echo "Generated files:"
        ls -la outputs/ || echo "No outputs directory"
        find outputs/ -type f -exec echo "File: {}" \; 2>/dev/null || echo "No files found"

    - name: Upload generated video
      uses: actions/upload-artifact@v4
      with:
        name: generated-video
        path: outputs/
        retention-days: 30
        if-no-files-found: warn
