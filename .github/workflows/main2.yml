name: AI Image Generator

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'Image prompt'
        required: true
        default: 'A beautiful sunset'

jobs:
  image-generator:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flask pillow requests torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        pip install diffusers accelerate transformers

    - name: Install Ngrok
      run: |
        wget -q -O ngrok.tgz https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz
        tar -xzf ngrok.tgz
        chmod +x ngrok
        sudo mv ngrok /usr/local/bin/
        ngrok version

    - name: Setup Ngrok authentication
      run: |
        ngrok authtoken "${{ secrets.NGROK_AUTH_TOKEN }}"
        echo "âœ… Ngrok configured"

    - name: Create Flask application
      run: |
        mkdir -p web-app/outputs
        cat > web-app/app.py << 'EOF'
from flask import Flask, request, jsonify, send_file
import torch
from diffusers import StableDiffusionPipeline
import io
import os
from datetime import datetime
import threading

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max-limit

# Global variable for the model
pipe = None
model_loaded = False

def load_model():
    """Load the AI model in a separate thread"""
    global pipe, model_loaded
    try:
        print("ðŸ”„ Loading AI model...")
        # Use a smaller model for faster loading
        pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        model_loaded = True
        print("âœ… AI model loaded successfully!")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")

@app.route('/health')
def health():
    return jsonify({
        "status": "healthy", 
        "model_loaded": model_loaded,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/generate', methods=['POST'])
def generate_image():
    if not model_loaded:
        return jsonify({"error": "Model still loading, please try again in 30 seconds"}), 503
    
    try:
        data = request.get_json()
        prompt = data.get('prompt', 'A beautiful landscape')
        
        print(f"ðŸŽ¨ Generating image for: {prompt}")
        
        # Generate image
        image = pipe(prompt, num_inference_steps=20, guidance_scale=7.5).images[0]
        
        # Save image
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"image_{timestamp}.png"
        filepath = os.path.join('outputs', filename)
        
        image.save(filepath)
        
        # Return image as response
        img_io = io.BytesIO()
        image.save(img_io, 'PNG')
        img_io.seek(0)
        
        return send_file(img_io, mimetype='image/png', as_attachment=True, download_name=filename)
        
    except Exception as e:
        print(f"âŒ Generation error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/test')
def test():
    """Test endpoint that returns a simple generated image"""
    if not model_loaded:
        return jsonify({"error": "Model still loading"}), 503
    
    try:
        image = pipe("a beautiful sunset over mountains", num_inference_steps=10).images[0]
        img_io = io.BytesIO()
        image.save(img_io, 'PNG')
        img_io.seek(0)
        return send_file(img_io, mimetype='image/png')
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    # Start model loading in background
    thread = threading.Thread(target=load_model)
    thread.daemon = True
    thread.start()
    
    app.run(host='0.0.0.0', port=5000, debug=False)
EOF

    - name: Start Flask server and load model
      run: |
        cd web-app
        python app.py &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        
        # Wait longer for server and model to load
        echo "â³ Waiting for server and AI model to load (this can take 2-5 minutes)..."
        for i in {1..60}; do
          if curl -s http://localhost:5000/health > /dev/null 2>&1; then
            echo "âœ… Server is running after $((i*5)) seconds"
            # Check if model is loaded
            response=$(curl -s http://localhost:5000/health)
            if echo "$response" | grep -q "true"; then
              echo "ðŸ¤– AI model loaded and ready!"
              break
            else
              echo "â³ Model still loading..."
            fi
          fi
          if [ $i -eq 60 ]; then
            echo "âŒ Server/model failed to start properly"
            exit 1
          fi
          sleep 5
        done

    - name: Start Ngrok tunnel
      run: |
        echo "ðŸŒ Starting Ngrok tunnel..."
        ngrok http 5000 --log=stdout > ngrok.log 2>&1 &
        echo "NGROK_PID=$!" >> $GITHUB_ENV
        sleep 10

    - name: Get Ngrok public URL
      id: ngrok_url
      run: |
        echo "ðŸ”— Getting Ngrok URL..."
        max_attempts=10
        for i in $(seq 1 $max_attempts); do
          URL=$(curl -s http://localhost:4040/api/tunnels 2>/dev/null | grep -o 'https://[^"]*\.ngrok\.io' | head -1)
          if [ -n "$URL" ]; then
            echo "âœ… Ngrok URL found: $URL"
            echo "ngrok_url=$URL" >> $GITHUB_OUTPUT
            break
          fi
          sleep 3
        done
        
        if [ -z "$URL" ]; then
          echo "âŒ Could not get Ngrok URL"
          exit 1
        fi

    - name: Display connection information
      run: |
        echo ""
        echo "âœ¨ ========================================="
        echo "âœ¨   AI IMAGE GENERATOR READY!"
        echo "âœ¨ ========================================="
        echo "âœ¨ Public URL: ${{ steps.ngrok_url.outputs.ngrok_url }}"
        echo "âœ¨ Health Check: ${{ steps.ngrok_url.outputs.ngrok_url }}/health"
        echo "âœ¨ Test Image: ${{ steps.ngrok_url.outputs.ngrok_url }}/test"
        echo "âœ¨ ========================================="
        echo ""
        echo "ðŸ“ How to use:"
        echo "   Send a POST request to: ${{ steps.ngrok_url.outputs.ngrok_url }}/generate"
        echo "   With JSON body: {\"prompt\": \"your image description here\"}"

    - name: Keep service running
      run: |
        echo "ðŸ• AI Image Generator active for 20 minutes..."
        echo "ðŸ“Š Monitoring service status..."
        
        for i in {1..120}; do
          minutes=$((i/6))
          seconds=$((i%6*10))
          echo "[$(date +%H:%M:%S)] Active: ${minutes}m ${seconds}s"
          
          # Check server health
          if curl -s "${{ steps.ngrok_url.outputs.ngrok_url }}/health" > /dev/null; then
            count=$(find web-app/outputs -name "*.png" 2>/dev/null | wc -l)
            echo "   âœ… Service healthy | Images: $count"
          else
            echo "   âŒ Service unavailable"
          fi
          
          sleep 10
        done

    - name: Upload generated images
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: ai-images
        path: web-app/outputs/
        retention-days: 30
        if-no-files-found: ignore

    - name: Cleanup
      if: always()
      run: |
        echo "ðŸ§¹ Cleaning up..."
        # Kill processes
        pkill -f "python.*app.py" || true
        pkill -f ngrok || true
        sleep 2
        
        # Show final stats
        echo "ðŸ“Š Final statistics:"
        find web-app/outputs -name "*.png" 2>/dev/null | wc -l | xargs echo "Total images generated:"
